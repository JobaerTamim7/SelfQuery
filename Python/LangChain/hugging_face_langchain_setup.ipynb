{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f70dfdc8",
   "metadata": {},
   "source": [
    "## **Setup Logger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f04e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import Logger\n",
    "\n",
    "formatter = logging.Formatter(\n",
    "    '[%(levelname)-5s] %(asctime)s: %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "console_logger: Logger = logging.getLogger(\"console_logger\")\n",
    "console_logger.setLevel(logging.INFO)\n",
    "console_logger.propagate = False\n",
    "console_logger.handlers.clear()\n",
    "\n",
    "if not console_logger.hasHandlers():\n",
    "    console_handler = logging.StreamHandler() # Create a handler that writes log messages to the console (standard output).\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    console_logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d982e24",
   "metadata": {},
   "source": [
    "## **Online Mode**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fad8bd",
   "metadata": {},
   "source": [
    "### **Set up environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36392b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] 2025-09-30 21:50:23: HuggingFace API token loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_sec_key: str | None = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "if hf_sec_key is None:\n",
    "    raise ValueError(\"HUGGINGFACEHUB_API_TOKEN is not set in environment variables.\")\n",
    "\n",
    "console_logger.info(\"HuggingFace API token loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8c9740",
   "metadata": {},
   "source": [
    "### **Login in HuggingFaceHub**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ddc9043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] 2025-09-30 21:50:27: Logged in to Hugging Face Hub successfully.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "try:\n",
    "    login(token=hf_sec_key)\n",
    "    console_logger.info(\"Logged in to Hugging Face Hub successfully.\")\n",
    "except Exception as e:\n",
    "    console_logger.error(f\"Failed to log in to Hugging Face Hub: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504d29f",
   "metadata": {},
   "source": [
    "### **Making LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc6b960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] 2025-09-30 21:50:31: Successfully created HuggingFaceEndpoint for model: deepseek-ai/DeepSeek-R1-0528\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "repo_id = \"deepseek-ai/DeepSeek-R1-0528\"\n",
    "\n",
    "try:  \n",
    "    hf_llm = HuggingFaceEndpoint(\n",
    "        model=repo_id,\n",
    "        task=\"conversational\",\n",
    "        temperature=0.7,\n",
    "        huggingfacehub_api_token=hf_sec_key,\n",
    "        provider=\"auto\"\n",
    "    )\n",
    "    console_logger.info(f\"Successfully created HuggingFaceEndpoint for model: {repo_id}\")\n",
    "except Exception as e:\n",
    "    console_logger.error(f\"Error creating HuggingFaceEndpoint: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1b1f2b",
   "metadata": {},
   "source": [
    "*Note:* Currently, the HuggingFaceEndpoint class internally uses the text_generation method from the huggingface_hub InferenceClient. This will fail for models that no longer have a provider supporting the text-generation task â€” thence this is not a LangChain issue. As a workaround, you can wrap the HuggingFaceEndpoint in a ChatHuggingFace instance and leverage the conversational task instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278da785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] 2025-09-30 21:50:33: ChatHuggingFace model initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "chat_model = ChatHuggingFace(llm=hf_llm) # Wrap the HuggingFaceEndpoint in a ChatHuggingFace instance\n",
    "console_logger.info(\"ChatHuggingFace model initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2500fd1",
   "metadata": {},
   "source": [
    "### **Building prompt and chain + Invoke or Stream from chain and Output result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e78aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking \"What is quantum computing?\" I need to provide a concise yet informative answer. Since the assistant is supposed to be helpful and factual, I'll stick to the essentials without fluff. \n",
      "\n",
      "Hmm, I recall quantum computing uses quantum bits or qubits. Unlike classical bits that are 0 or 1, qubits can be in a superposition of states. That's a key difference. Also, entanglement allows qubits to be interconnected in ways classical bits can't. These properties enable quantum computers to perform complex calculations much faster for certain problems. \n",
      "\n",
      "The user might be a beginner, so I should avoid jargon where possible. But since they asked directly, they probably want the core concepts. Potential applications like cryptography or simulation could be worth mentioning briefly. \n",
      "\n",
      "I must resist adding tangential detailsâ€”stick to the question. The assistant's reply needs to be short but complete enough to answer \"what\" it is. Got it: superposition, entanglement, and the computational advantage for specific tasks. No speculation beyond verified facts. \n",
      "\n",
      "Final phrasing must be clear: qubits' unique properties + purpose (solving impractical problems). No need for examples unless asked.\n",
      "</think>\n",
      "Quantum computing is a type of computation that utilizes **quantum bits (qubits)** instead of classical bits. Unlike classical bits (which are either 0 or 1), qubits can exist in a **superposition** of both 0 and 1 simultaneously. They also leverage **entanglement** (a strong correlation between qubits) and **quantum interference**. \n",
      "\n",
      "This allows quantum computers to perform complex calculations at speeds **exponentially faster** than classical computers *for specific problems*, such as:  \n",
      "- Factoring large numbers (impacting cryptography).  \n",
      "- Simulating quantum systems (e.g., chemical reactions).  \n",
      "- Optimizing complex systems (logistics, finance).  \n",
      "\n",
      "However, quantum computers are **not general-purpose replacements** for classical computers and face challenges like error rates and decoherence.  \n",
      "\n",
      "ðŸ’¡ **In short**: Quantum computing uses quantum mechanics to solve certain intractable problems much faster than classical computers."
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful assistant that helps people find information. You answer as concisely as possible. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "# response: str = chain.invoke({\"question\": \"What is quantum computing?\"})  # This method directly invokes the chain and returns the output.\n",
    "\n",
    "for chunk in chain.stream({\"question\": \"What is quantum computing?\"}):\n",
    "    response = chunk\n",
    "    print(chunk, end='', flush=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f9146",
   "metadata": {},
   "source": [
    "## **Offline Mode**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b03bdcd",
   "metadata": {},
   "source": [
    "### **Pulling model locally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8c02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7802c9bee1f14ef286b0dcc69bf3bc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8678522f6fe43c9a6224b4a02588fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830a08fa25194de5b7ac6e24b6406689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e34bc1ade046468b0b75d87cc8aedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1304194e374d90b7748a5b405a026e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17eb1cac19a4918be36d09ea58b6329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb36f2c231c4505991eb82a5e2055b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "[INFO ] 2025-09-30 21:59:31: HuggingFacePipeline model initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "try:\n",
    "    hf_llm_offline: HuggingFacePipeline = HuggingFacePipeline.from_model_id(\n",
    "        model_id=\"gpt2\",\n",
    "        task=\"text-generation\",\n",
    "        model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
    "    )\n",
    "    console_logger.info(\"HuggingFacePipeline model initialized successfully.\")\n",
    "except Exception as e:\n",
    "    console_logger.error(f\"Error initializing HuggingFacePipeline: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddc1583",
   "metadata": {},
   "source": [
    "### **Invoke or Stream result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255350ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking for a simple explanation of the theory of relativity. They probably want a clear, jargon-free overview without getting lost in complex math. \n",
      "\n",
      "Hmm, judging by \"simple terms,\" they might be a curious non-scientistâ€”maybe a student, a lifelong learner, or just someone who heard the term and wanted clarity. No need to assume prior physics knowledge here. \n",
      "\n",
      "Breaking this down: I should cover both special and general relativity since people often conflate them. Key points to hit: space-time unity, speed of light as constant, and gravityâ€™s role in curving space. Keep metaphors concrete (\"stretchy fabric\" for space-time, headlights on a car for light speed). \n",
      "\n",
      "Ah, and must emphasize the theoryâ€™s big-picture impactâ€”how it changed Newtonian viewsâ€”without oversimplifying. User might appreciate that context. \n",
      "\n",
      "Also... no equations! Unless they ask later. Short sentences. \"Faster time moves slower\" could confuse; \"time slows as speed increases\" is clearer. \n",
      "\n",
      "*Double-checking mental notes*: \n",
      "- Special relativity = speed & light \n",
      "- General = gravity & acceleration \n",
      "- Mention GPS as a real-world application? Maybe overkill for \"simple terms.\" Skip unless asked. \n",
      "\n",
      "Waitâ€”should I address if this conflicts with quantum mechanics? No, thatâ€™s a tangent. Stay focused. User said \"simple.\" \n",
      "\n",
      "Done. Structure: \n",
      "1. One-sentence definition \n",
      "2. Split special/general \n",
      "3. Wrap with why it matters \n",
      "... and keep it under 4 sentences total. Tight.\n",
      "</think>\n",
      "The theory of relativity, proposed by Albert Einstein, consists of two parts:  \n",
      "\n",
      "1. **Special Relativity (1905)**:  \n",
      "   - **Space and time are interconnected** into a single \"space-time.\"  \n",
      "   - **The speed of light** is constant for all observers, regardless of motion.  \n",
      "   - **As you approach light speed**, time slows down (time dilation), lengths shorten, and mass increases.  \n",
      "\n",
      "2. **General Relativity (1915)**:  \n",
      "   - **Gravity is not a force** but a curvature of space-time caused by mass and energy.  \n",
      "   - Objects (like planets) move along curved paths in this warped space-time, which we perceive as gravity.  \n",
      "\n",
      "**In a nutshell**: Relativity shows that time, space, and gravity aren't fixed â€” they depend on your speed and the presence of mass/energy. It replaced Newtonâ€™s ideas for extreme speeds or strong gravity."
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate,SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt1 = PromptTemplate.from_template(\"Explain the following topic in simple terms: {question}\")\n",
    "\n",
    "offline_chain = prompt1 | hf_llm_offline | StrOutputParser()\n",
    "\n",
    "for chunk in chain.stream({\"question\": \"Explain the theory of relativity in simple terms.\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chain (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
