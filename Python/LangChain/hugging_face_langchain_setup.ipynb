{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f70dfdc8",
   "metadata": {},
   "source": [
    "## **Setup Logger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f04e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import Logger\n",
    "\n",
    "formatter = logging.Formatter(\n",
    "    '[%(levelname)-5s] %(asctime)s: %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "console_logger: Logger = logging.getLogger(\"console_logger\")\n",
    "console_logger.setLevel(logging.INFO)\n",
    "console_logger.propagate = False\n",
    "console_logger.handlers.clear()\n",
    "\n",
    "if not console_logger.hasHandlers():\n",
    "    console_handler = logging.StreamHandler() # Create a handler that writes log messages to the console (standard output).\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    console_logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d982e24",
   "metadata": {},
   "source": [
    "## **Online Mode**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fad8bd",
   "metadata": {},
   "source": [
    "### **Set up environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36392b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] 2025-10-01 10:33:18: HuggingFace API token loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_sec_key: str | None = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "if hf_sec_key is None:\n",
    "    raise ValueError(\"HUGGINGFACEHUB_API_TOKEN is not set in environment variables.\")\n",
    "\n",
    "console_logger.info(\"HuggingFace API token loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8c9740",
   "metadata": {},
   "source": [
    "### **Login in HuggingFaceHub**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ddc9043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] 2025-10-01 10:33:18: Logged in to Hugging Face Hub successfully.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "try:\n",
    "    login(token=hf_sec_key)\n",
    "    console_logger.info(\"Logged in to Hugging Face Hub successfully.\")\n",
    "except Exception as e:\n",
    "    console_logger.error(f\"Failed to log in to Hugging Face Hub: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504d29f",
   "metadata": {},
   "source": [
    "### **Making LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fc6b960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] 2025-10-01 10:33:18: Successfully created HuggingFaceEndpoint for model: deepseek-ai/DeepSeek-R1-0528\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "repo_id = \"deepseek-ai/DeepSeek-R1-0528\"\n",
    "\n",
    "try:  \n",
    "    hf_llm = HuggingFaceEndpoint(\n",
    "        model=repo_id,\n",
    "        task=\"conversational\",\n",
    "        temperature=0.7,\n",
    "        huggingfacehub_api_token=hf_sec_key,\n",
    "        provider=\"auto\"\n",
    "    )\n",
    "    console_logger.info(f\"Successfully created HuggingFaceEndpoint for model: {repo_id}\")\n",
    "except Exception as e:\n",
    "    console_logger.error(f\"Error creating HuggingFaceEndpoint: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1b1f2b",
   "metadata": {},
   "source": [
    "*Note:* Currently, the HuggingFaceEndpoint class internally uses the text_generation method from the huggingface_hub InferenceClient. This will fail for models that no longer have a provider supporting the text-generation task — thence this is not a LangChain issue. As a workaround, you can wrap the HuggingFaceEndpoint in a ChatHuggingFace instance and leverage the conversational task instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "278da785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] 2025-10-01 10:33:18: ChatHuggingFace model initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "chat_model = ChatHuggingFace(llm=hf_llm) # Wrap the HuggingFaceEndpoint in a ChatHuggingFace instance\n",
    "console_logger.info(\"ChatHuggingFace model initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2500fd1",
   "metadata": {},
   "source": [
    "### **Building prompt and chain + Invoke or Stream from chain and Output result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f6e78aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking about quantum computing. I need to provide a concise yet clear explanation. From what I've studied, quantum computing fundamentally differs from classical computing. Classical computers use bits as 0s or 1s, while quantum computers use quantum bits, or qubits, which can be in multiple states at once due to superposition. \n",
      "\n",
      "The user might be a student or someone new to the concept, so avoiding jargon while still being accurate is key. They probably need the core principles explained simply. I recall entanglement as another crucial aspect, where qubits are linked together so one state immediately affects others. That enables faster computations. \n",
      "\n",
      "Quantum algorithms like Shor's for factoring or Grover's for searching are important examples, but maybe too detailed here. The answer should focus on qubits, superposition, and entanglement as the backbone. Also, noting that quantum computers are specialized for specific complex problems could address why they're significant. \n",
      "\n",
      "I wonder if the user has a deeper interest—perhaps academic or related to emerging tech fields. But without more context, sticking to the basics is safer. No need to speculate; just answer what's asked. \n",
      "\n",
      "The challenge is balancing simplicity with enough depth. Should I mention decoherence? Probably less relevant for this definition. Applications like cryptography or drug discovery might overcomplicate it. Focus on the mechanism: qubits, superposition, entanglement → parallel processing. \n",
      "\n",
      "Double-checking: yes, that covers the essence. Conciseness is prioritized per the assistant's guidelines. Keep it to two sentences unless more is essential. Done right, this should clarify without overwhelming.\n",
      "</think>\n",
      "**Quantum computing** is a type of computing that uses the principles of quantum mechanics (like superposition, entanglement, and quantum interference) to process information. Instead of traditional bits (0 or 1), it uses **quantum bits (qubits)** that can represent both 0 and 1 simultaneously. This allows quantum computers to explore many possibilities at once and solve certain complex problems—like simulating molecules, breaking specific encryption, or optimizing large systems—much faster than classical computers in theory."
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful assistant that helps people find information. You answer as concisely as possible. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "# response: str = chain.invoke({\"question\": \"What is quantum computing?\"})  # This method directly invokes the chain and returns the output.\n",
    "\n",
    "for chunk in chain.stream({\"question\": \"What is quantum computing?\"}):\n",
    "    response = chunk\n",
    "    print(chunk, end='', flush=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f9146",
   "metadata": {},
   "source": [
    "## **Offline Mode**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e49c52",
   "metadata": {},
   "source": [
    "*Note*: Before starting we need to know few basic details. First the model loaded in the local machine might take much space. This models are saved in the cache of huggingface_hub generally in our main drive C: or macOS main SSD. To save system space we can change the cache directory by setting the environment variable `HF_HOME` to a directory in a drive with more space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b03bdcd",
   "metadata": {},
   "source": [
    "### **Pulling model locally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b8c02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "[INFO ] 2025-10-01 10:33:50: HuggingFacePipeline model initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "try:\n",
    "    hf_llm_offline: HuggingFacePipeline = HuggingFacePipeline.from_model_id(\n",
    "        model_id=\"gpt2\",\n",
    "        task=\"text-generation\",\n",
    "        device_map=\"mps\",\n",
    "        model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
    "    )\n",
    "    console_logger.info(\"HuggingFacePipeline model initialized successfully.\")\n",
    "except Exception as e:\n",
    "    console_logger.error(f\"Error initializing HuggingFacePipeline: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddc1583",
   "metadata": {},
   "source": [
    "### **Invoke or Stream result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "255350ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user wants a simple explanation of the theory of relativity. Let me start by recalling the two parts: special and general relativity. \n",
      "\n",
      "Special relativity deals with high speeds and no gravity. Key points: speed of light is constant, time and space are relative. Examples help—like the train example or twins aging differently.\n",
      "\n",
      "General relativity includes gravity. Space-time bending around masses. The ball on a trampoline analogy for planets orbiting. GPS as a real-world application.\n",
      "\n",
      "User might not have a physics background. Avoid jargon—use everyday analogies. Keep it concise but cover both theories. Check if mentioning Einstein adds context but since they asked for theory, focus on concepts.\n",
      "\n",
      "Potential deeper needs: could be a student preparing for a test, or someone curious about popular science. Either way, clarity is key. No equations, just principles and effects. Make sure to emphasize that it's about perspective and gravity's effect on space-time.\n",
      "</think>\n",
      "The theory of relativity, proposed by Albert Einstein, has two main parts:\n",
      "\n",
      "1.  **Special Relativity (1905):**  \n",
      "    Deals with objects moving at *constant high speeds* (especially near light speed) in a straight line.  \n",
      "    Key ideas:  \n",
      "    - **The laws of physics are the same for everyone.**  \n",
      "    - **The speed of light is constant** (approx. 300,000 km/s) for *all* observers, no matter how fast they move.  \n",
      "    - When things move close to light speed:  \n",
      "        - **Time slows down** (time dilation).  \n",
      "        - **Length shortens** (length contraction).  \n",
      "        - **Mass and energy are equivalent** (E=mc²).\n",
      "\n",
      "2.  **General Relativity (1915):**  \n",
      "    Adds *gravity and acceleration* to the picture.  \n",
      "    Key idea:  \n",
      "    - **Gravity isn't a force pulling things.** Instead, massive objects (like stars and planets) **warp space and time** around them, forming a \"fabric\" called **spacetime.**  \n",
      "    - Objects (like planets) follow curved paths (orbits) because they are moving along the curves in this warped spacetime.  \n",
      "    *Analogy:* Think of placing a heavy ball on a trampoline. It makes a dent. A smaller ball rolled nearby will curve around the dent, not because of a \"force,\" but because it's rolling along the curved surface.\n",
      "\n",
      "**In super simple terms:**  \n",
      "- **Special:** Fast motion changes time, length, and mass; light speed is absolute.  \n",
      "- **General:** Gravity = Curvature of spacetime caused by mass/energy (\"mass tells spacetime how to curve; curved spacetime tells mass how to move\")."
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate,SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt1 = PromptTemplate.from_template(\"Explain the following topic in simple terms: {question}\")\n",
    "\n",
    "offline_chain = prompt1 | hf_llm_offline | StrOutputParser()\n",
    "\n",
    "for chunk in chain.stream({\"question\": \"Explain the theory of relativity in simple terms.\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
