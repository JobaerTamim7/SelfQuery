{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35c2ce4",
   "metadata": {},
   "source": [
    "## **Setup Rich Text Editor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c06cf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.pretty import pprint\n",
    "from rich import print as fprint\n",
    "from rich import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9a5ff",
   "metadata": {},
   "source": [
    "## **Load PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "198e31c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'producer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'PyPDF2'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'creator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'PyPDF'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'creationdate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'subject'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Neural Information Processing Systems http://nips.cc/'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'publisher'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Curran Associates, Inc.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'language'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'en-US'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'created'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2017'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eventtype'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Poster'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'description-abstract'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Attention is All you Need'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2017'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'moddate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2018-02-12T21:22:10-08:00'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'published'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2017'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Conference Proceedings'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'firstpage'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'5998'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'book'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Advances in Neural Information Processing Systems 30'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'editors'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'lastpage'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'6008'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../assets/NIPS-2017-attention-is-all-you-need-Paper.pdf'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'total_pages'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'producer'\u001b[0m: \u001b[32m'PyPDF2'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'creator'\u001b[0m: \u001b[32m'PyPDF'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'creationdate'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'subject'\u001b[0m: \u001b[32m'Neural Information Processing Systems http://nips.cc/'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'publisher'\u001b[0m: \u001b[32m'Curran Associates, Inc.'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'language'\u001b[0m: \u001b[32m'en-US'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'created'\u001b[0m: \u001b[32m'2017'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'eventtype'\u001b[0m: \u001b[32m'Poster'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'description-abstract'\u001b[0m: \u001b[32m'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'Attention is All you Need'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'date'\u001b[0m: \u001b[32m'2017'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'moddate'\u001b[0m: \u001b[32m'2018-02-12T21:22:10-08:00'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'published'\u001b[0m: \u001b[32m'2017'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'Conference Proceedings'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'firstpage'\u001b[0m: \u001b[32m'5998'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'book'\u001b[0m: \u001b[32m'Advances in Neural Information Processing Systems 30'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'description'\u001b[0m: \u001b[32m'Paper accepted and presented at the Neural Information Processing Systems Conference \u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttp://nips.cc/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'editors'\u001b[0m: \u001b[32m'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'author'\u001b[0m: \u001b[32m'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'lastpage'\u001b[0m: \u001b[32m'6008'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'../assets/NIPS-2017-attention-is-all-you-need-Paper.pdf'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'total_pages'\u001b[0m: \u001b[1;36m11\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'page'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'page_label'\u001b[0m: \u001b[32m'1'\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mpage_content\u001b[0m=\u001b[32m'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory \u001b[0m\u001b[32m[\u001b[0m\u001b[32m12\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and gated recurrent \u001b[0m\u001b[32m[\u001b[0m\u001b[32m7\u001b[0m\u001b[32m]\u001b[0m\u001b[32m neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation \u001b[0m\u001b[32m[\u001b[0m\u001b[32m 29, 2, 5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures \u001b[0m\u001b[32m[\u001b[0m\u001b[32m31, 21, 13\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNIPS 2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Long Beach, CA, USA.'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "loader: PyPDFLoader = PyPDFLoader(file_path=\"../assets/NIPS-2017-attention-is-all-you-need-Paper.pdf\")\n",
    "\n",
    "docs: list[Document] = loader.load()\n",
    "pprint(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae9bdcf",
   "metadata": {},
   "source": [
    "## **Semantic Chunking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9475dde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                     ID              SIZE      MODIFIED     \n",
      "llama3.1:8b              46e0c10c039e    4.9 GB    2 hours ago     \n",
      "nomic-embed-text:v1.5    0a109f422b47    274 MB    26 hours ago    \n",
      "deepseek-r1:1.5b         e0979632db5a    1.1 GB    7 days ago      \n",
      "llava:13b                0d0eb4d7f485    8.0 GB    2 months ago    \n",
      "mistral:latest           3944fe81ec14    4.1 GB    2 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1a0627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Length of chunks :  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Length of chunks :  \u001b[1;36m31\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_ollama import  OllamaEmbeddings\n",
    "\n",
    "embeddings: OllamaEmbeddings = OllamaEmbeddings(model=\"nomic-embed-text:v1.5\")\n",
    "semantic_splitter: SemanticChunker = SemanticChunker(embeddings=embeddings)\n",
    "\n",
    "docs_chunks: list[Document] = semantic_splitter.split_documents(documents=docs)\n",
    "fprint(\"Length of chunks : \", len(docs_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27401f30",
   "metadata": {},
   "source": [
    "## **Creating Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7b42d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma.vectorstores import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=docs_chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"../DB/transformer_paper_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab263006",
   "metadata": {},
   "source": [
    "## **Built in RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02991c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transformers, based on the provided context, are model architectures that use a multi-layer, multi-head self-attention mechanism and fully connected feed-forward networks as sub-layers. They employ residual connections and layer normalization in each of their layers to facilitate these connections. The dimensions of the output of all sub-layers and embedding layers are set to 512 (dmodel = 512). In the decoder, an additional third sub-layer is added that performs multi-head attention over the output of the encoder stack. To prevent positions from attending to subsequent positions in the decoder stack, self-attention sub-layers are modified with position masking and offset embeddings."
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "llm = ChatOllama(model=\"mistral:latest\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant. Use only the context below to answer. If unsure, say \"Based on provided info, I don't know.\"\n",
    "    Context: {context}\n",
    "    Question: {input}\n",
    "    Answer:\"\"\"\n",
    ")\n",
    "\n",
    "stuff_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    ") # A Runnable object that will build the prompt with context input. Then llm will create the response.\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\":2}),\n",
    "    combine_docs_chain=stuff_chain\n",
    ") # A Runnable Object that will search for similarity and after finding top k similarity pass to stuff chain\n",
    "\n",
    "\n",
    "main_chain = retrieval_chain | RunnableLambda(lambda x : x.get(\"answer\", \"\"))\n",
    "\n",
    "query = \"What are transformers?\"\n",
    "\n",
    "for chunk in main_chain.stream({\"input\":query}):\n",
    "    print(chunk,end=\"\",flush=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
